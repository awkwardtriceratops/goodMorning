<html><head><link rel="stylesheet" type="text/css" href="css.css"></head><h1 id="artitle">What Keeps Facebook’s Election Security Chief Up at Night?<br></h1><p id="artcont">The social media company’s head of cybersecurity policy on “perception hacks” and what it will take to have an authentic election. By Charlie Warzel Opinion Writer at Large Facebook is not the entire internet. But it does reflect and account for some of the greater web’s chaos. With just days to go, hyperpartisan pages on the platform are churning out propaganda to millions of followers. In recent weeks, malicious actors both foreign and domestic have attempted to use inauthentic networks to push narratives to sow confusion and division. Others, including President Trump and his campaign, have used the platform to spread false information about voting while some partisans try to undermine the public’s faith in the U.S. election system. Then there are the conspiracy theorists and the long-running battle Facebook continues to fight against pandemic-related misinformation and disinformation. Which is to say that all eyes are on Facebook. The security of the platform from outside interference as well as domestic manipulation is a crucial factor in assuring a fair and free election. At the head of that effort is Nathaniel Gleicher, Facebook’s head of cybersecurity policy. I spoke with him on Friday afternoon. This is a condensed and edited version of our conversation for clarity. What’s your specific role with the platform as it relates to the upcoming election? My work is to find and deal with two kinds of threats: cybersecurity, which is hacking, phishing and exploiting Facebook’s technical assets. The other is influence operations, which is both foreign (Russia, Iran, China) and domestic actors manipulating public debate with disinformation or in other ways. So you’re not involved in content moderation? Your team doesn’t take down specific posts because they violate a rule? We tend to treat public debate problems online as one thing. But they’re very different. Camille François, a disinformation researcher, has a useful model — You can break the threat into three parts: actors, behaviors and content. We are on the actor and behavior team. That’s intentional because in influence operations content isn’t always a good signal for what’s happening. We’ve seen Russian actors intentionally use content posted by innocent Americans. We see other people post and share content from Russian campaigns. It doesn’t mean they’re actually connected. In fact, most times they’re not. But that is the point. These foreign operations want to look more powerful than they are. 