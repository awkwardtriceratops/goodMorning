<html><head><link rel="stylesheet" type="text/css" href="css.css"></head><h1 id="artitle">What Keeps Facebook’s Election Security Chief Up at Night?<br></h1><p id="artcont">The social media company’s head of cybersecurity policy on “perception hacks” and what it will take to have an authentic election. By Charlie Warzel Opinion Writer at Large Facebook is not the entire internet. But it does reflect and account for some of the greater web’s chaos. With just days to go, hyperpartisan pages on the platform are churning out propaganda to millions of followers. In recent weeks, malicious actors both foreign and domestic have attempted to use inauthentic networks to push narratives to sow confusion and division. Others, including President Trump and his campaign, have used the platform to spread false information about voting while some partisans try to undermine the public’s faith in the U.S. election system. Then there are the conspiracy theorists and the long-running battle Facebook continues to fight against pandemic-related misinformation and disinformation. Which is to say that all eyes are on Facebook. The security of the platform from outside interference as well as domestic manipulation is a crucial factor in assuring a fair and free election. At the head of that effort is Nathaniel Gleicher, Facebook’s head of cybersecurity policy. I spoke with him on Friday afternoon. This is a condensed and edited version of our conversation for clarity. What’s your specific role with the platform as it relates to the upcoming election? My work is to find and deal with two kinds of threats: cybersecurity, which is hacking, phishing and exploiting Facebook’s technical assets. The other is influence operations, which is both foreign (Russia, Iran, China) and domestic actors manipulating public debate with disinformation or in other ways. So you’re not involved in content moderation? Your team doesn’t take down specific posts because they violate a rule? We tend to treat public debate problems online as one thing. But they’re very different. Camille François, a disinformation researcher, has a useful model — You can break the threat into three parts: actors, behaviors and content. We are on the actor and behavior team. That’s intentional because in influence operations content isn’t always a good signal for what’s happening. We’ve seen Russian actors intentionally use content posted by innocent Americans. We see other people post and share content from Russian campaigns. It doesn’t mean they’re actually connected. In fact, most times they’re not. But that is the point. These foreign operations want to look more powerful than they are. There’s been a lot of debate on this topic. Namely, that the reach and potency of foreign interference is overstated or at least over-covered in the press and that the biggest problems are actually organic and domestic. One thing Facebook started doing after I joined is we began publicly announcing coordinated inauthentic behavior (a somewhat vague term that means using fake accounts to artificially boost information designed to mislead) takedowns. We’ve found more than 100 of these in the last three years and we announce them and publicly share info and give this to third-party researchers so they can give their own independent assessment of what’s happening. As a result, these operations are getting caught earlier and reaching fewer people and having less impact. That’s also because government organizations, civil society groups and journalists are all helping to identify this. What that means is that their tactics are shifting. Foreign adversaries are doing things like luring real journalists to create divisive content. Are these malicious actors trying to use fear to get us to manipulate ourselves? Influence operations are essentially weaponized uncertainty. They’re trying to get us all to be afraid. Russian actors want us to think there’s a Russian under every rock. Foreign actors want us to think they have completely compromised our systems, and there isn’t evidence for that. In a situation like this, having the facts becomes extremely useful. Being able to see the effectiveness or ineffectiveness of these campaigns is useful. It’s a tool we can use to help protect ourselves. We know they’re planning to play on our fears. They’re trying to trick us into doing this to ourselves, and we don’t have to take the bait. It seems we as a nation are our own worst enemy in this respect. It’s like you wake up in the morning on Election Day and the whole process is this black box. It feels like jumping off a cliff and you land at the bottom when the votes are counted and you don’t really see the things that happened along the way. But really there’s a staircase you can take. There’s a bunch of steps. Voting starts, then officials begin counting ballots. There are controls and systems in place, and at the end you’ve made it to the bottom of the staircase. We need to do our part to show people the staircase and what happens in each moment to say, “There’s a plan to all this.” A threat actor wants to exploit the uncertainty in the election process to make us feel like the system is broken. But that’s harder to do if you can see the system. How do we protect ourselves and our democracy? One of the most effective countermeasures in all of this is an informed public. So we have to do the best — all of us, not just Facebook — to amplify authentic information. One of biggest differences between 2016 and 2020 is that you have teams in government, in tech, in civil society that understand the risks and challenges and are working together. We didn’t have this four years ago. What keeps you up at night? 