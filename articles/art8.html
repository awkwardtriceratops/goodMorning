<html><head><link rel="stylesheet" type="text/css" href="css.css"></head><h1 id="artitle">What Keeps Facebook’s Election Security Chief Up at Night?<br></h1><p id="artcont">The social media company’s head of cybersecurity policy on “perception hacks” and what it will take to have an authentic election. By Charlie Warzel Opinion Writer at Large Facebook is not the entire internet. But it does reflect and account for some of the greater web’s chaos. With just days to go, hyperpartisan pages on the platform are churning out propaganda to millions of followers. In recent weeks, malicious actors both foreign and domestic have attempted to use inauthentic networks to push narratives to sow confusion and division. Others, including President Trump and his campaign, have used the platform to spread false information about voting while some partisans try to undermine the public’s faith in the U.S. election system. Then there are the conspiracy theorists and the long-running battle Facebook continues to fight against pandemic-related misinformation and disinformation. Which is to say that all eyes are on Facebook. The security of the platform from outside interference as well as domestic manipulation is a crucial factor in assuring a fair and free election. At the head of that effort is Nathaniel Gleicher, Facebook’s head of cybersecurity policy. I spoke with him on Friday afternoon. This is a condensed and edited version of our conversation for clarity. What’s your specific role with the platform as it relates to the upcoming election? My work is to find and deal with two kinds of threats: cybersecurity, which is hacking, phishing and exploiting Facebook’s technical assets. The other is influence operations, which is both foreign (Russia, Iran, China) and domestic actors manipulating public debate with disinformation or in other ways. So you’re not involved in content moderation? Your team doesn’t take down specific posts because they violate a rule? We tend to treat public debate problems online as one thing. But they’re very different. Camille François, a disinformation researcher, has a useful model — You can break the threat into three parts: actors, behaviors and content. We are on the actor and behavior team. That’s intentional because in influence operations content isn’t always a good signal for what’s happening. We’ve seen Russian actors intentionally use content posted by innocent Americans. We see other people post and share content from Russian campaigns. It doesn’t mean they’re actually connected. In fact, most times they’re not. But that is the point. These foreign operations want to look more powerful than they are. There’s been a lot of debate on this topic. Namely, that the reach and potency of foreign interference is overstated or at least over-covered in the press and that the biggest problems are actually organic and domestic. One thing Facebook started doing after I joined is we began publicly announcing coordinated inauthentic behavior (a somewhat vague term that means using fake accounts to artificially boost information designed to mislead) takedowns. We’ve found more than 100 of these in the last three years and we announce them and publicly share info and give this to third-party researchers so they can give their own independent assessment of what’s happening. As a result, these operations are getting caught earlier and reaching fewer people and having less impact. That’s also because government organizations, civil society groups and journalists are all helping to identify this. What that means is that their tactics are shifting. Foreign adversaries are doing things like luring real journalists to create divisive content. Are these malicious actors trying to use fear to get us to manipulate ourselves? Influence operations are essentially weaponized uncertainty. They’re trying to get us all to be afraid. Russian actors want us to think there’s a Russian under every rock. Foreign actors want us to think they have completely compromised our systems, and there isn’t evidence for that. In a situation like this, having the facts becomes extremely useful. Being able to see the effectiveness or ineffectiveness of these campaigns is useful. It’s a tool we can use to help protect ourselves. We know they’re planning to play on our fears. They’re trying to trick us into doing this to ourselves, and we don’t have to take the bait. It seems we as a nation are our own worst enemy in this respect. It’s like you wake up in the morning on Election Day and the whole process is this black box. It feels like jumping off a cliff and you land at the bottom when the votes are counted and you don’t really see the things that happened along the way. But really there’s a staircase you can take. There’s a bunch of steps. Voting starts, then officials begin counting ballots. There are controls and systems in place, and at the end you’ve made it to the bottom of the staircase. We need to do our part to show people the staircase and what happens in each moment to say, “There’s a plan to all this.” A threat actor wants to exploit the uncertainty in the election process to make us feel like the system is broken. But that’s harder to do if you can see the system. How do we protect ourselves and our democracy? One of the most effective countermeasures in all of this is an informed public. So we have to do the best — all of us, not just Facebook — to amplify authentic information. One of biggest differences between 2016 and 2020 is that you have teams in government, in tech, in civil society that understand the risks and challenges and are working together. We didn’t have this four years ago. What keeps you up at night? A year ago we predicted some trends we thought we might see. A number of them have come to pass and we’re ahead of a few things. We’ve seen threat actors target smaller communities trying to hide from us. We predicted that bad actors would move from fake accounts to trying to target influencers. They’ve tried to do it but haven’t reached all that many people because we saw it coming. When we think of things to be worried about the first is that our elections system is very complex. And there are so many opportunities to leverage that complexity to run a perception hack. A perception hack is an attempt to create a perception that there is a large scale influence operation when in fact there is no evidence to support it. The other big piece is the very tense civic debate around the outcome as votes are counted. You can imagine malicious actors will try to accelerate that debate and that we’re certainly focused on that. Like efforts to claim without any basis that a spike in voting in a swing district or state is evidence of fraud and trying to use that to inspire or incite conflict. What you want to do is call it out right now ahead of time so that if they do it, people will say, “Hey, look at that claim. We were just hearing that claim might be made to hack our perception.” It’s part of why there is a large effort concentrated around debunking and prebunking. If an uninformed or rash or gullible public is working against its own interest, that seems potentially outside the scope of you and your team. 