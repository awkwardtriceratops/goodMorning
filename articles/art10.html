<html><head><link rel="stylesheet" type="text/css" href="css.css"></head><h1 id="artitle">When Algorithms Give Real Students Imaginary Grades<br></h1><p id="artcont">In-person final exams were canceled for thousands of students this spring, so computers stepped in — to disastrous effect. By Meredith Broussard Ms. Broussard is an artificial intelligence researcher at New York University. Isabel Castañeda’s first words were in Spanish. She spends every summer with relatives in Mexico. She speaks Spanish with her family at home. When her school, Westminster High in Colorado, closed for the pandemic in March, her Spanish literature class had just finished analyzing an entire novel in translation, Albert Camus’s “The Plague.” She got a 5 out of 5 on her Advanced Placement Spanish exam last year, following two straight years of A+ grades in Spanish class. And yet, she failed her International Baccalaureate Spanish exam this year. When she got her final results, Ms. Castañeda was shocked. “Everybody believed that I was going to score very high,” she told me. “Then, the scores came back and I didn’t even score a passing grade. I scored well below passing.” How did this happen? An algorithm assigned a grade to Ms. Castañeda and 160,000 other students. The International Baccalaureate — a global program that awards a prestigious diploma to students in addition to the one they receive from their high schools — canceled its usual in-person final exams because of the pandemic. Instead, it used an algorithm to “predict” students’ grades, based on an array of student information, including teacher-estimated grades and past performance by students in each school. Ms. Castañeda was not alone in receiving a surprising failing grade — tens of thousands of International Baccalaureate students protested their computer-assigned grades online and in person. High-achieving, low-income students were hit particularly hard: many took the exams expecting to earn college credit with their scores and save thousands of dollars on tuition. Nor was the International Baccalaureate the only organization to use a computer program to assign students grades amid the pandemic. The United Kingdom’s in-person A-level exams, which help determine which universities students go to, were also canceled and replaced with grades-by-algorithm. Students who were denied university entrance because of these imaginary grades took to the streets, chanting anti-algorithm slogans. Only after an uproar did the government change course, though many students were left in limbo without university admission. The lesson from these debacles is clear: Algorithms should not be used to assign student grades. And we should think much more critically about algorithmic decision-making overall, especially in education. The pandemic makes it tempting to imagine that social institutions like school can be replaced by technological solutions. They can’t. The bureaucrats who decided to use a computer to assign grades are guilty of a bias I call technochauvinism: the idea that technological solutions are superior. It’s usually accompanied by equally bogus notions like, “Computers make neutral decisions” or, “Computers are objective because their decisions are based on math.” Computers are excellent at doing math, but education is not math — it’s a social system. And algorithmic systems repeatedly fail at making social decisions. Algorithms can’t monitor or detect hate speech, they can’t replace social workers in public assistance programs, they can’t predict crime, they can’t determine which job applicants are more suited than others, they can’t do effective facial recognition, and they can’t grade essays or replace teachers. In the case of the International Baccalaureate program, grades could have been assigned based on the sample materials that students had already submitted by the time schools shut down. Instead, the organization decided to use an algorithm, which probably seemed like it would be cheaper and easier. 